{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation des librairies\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from river import base\n",
    "from river import metrics\n",
    "from river import stream\n",
    "# from codecarbon import OfflineEmissionsTracker\n",
    "from os import path\n",
    "import json\n",
    "from math import *\n",
    "\n",
    "# Choix de la cpu, gpu ou autre\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition de l'architecture du modèle\n",
    "class Architecture(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Sigmoid(),  # Class multi-label\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "\n",
    "# Définition et initialisation du modèle pour river, avec les fonctions learn_one et predict_one sur une seule instance du flux \n",
    "class NN(base.MultiLabelClassifier):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        feature_size=1006, #Taille des vecteurs en entrée\n",
    "        hidden_sizes=300,\n",
    "        label_size=20, #Label a prédire\n",
    "    ):\n",
    "\n",
    "        # SETTING UP THE MODEL\n",
    "        self.model = Architecture(\n",
    "            feature_size,hidden_sizes, label_size\n",
    "        ).to(device)\n",
    "        self.model = self.model.double() #Convertie le type de donnée en double\n",
    "        self.loss = nn.BCELoss() #Fc de perte\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "\n",
    "\n",
    "    #x -> Dico de caractéristique pour une instance de données\n",
    "    #y -> Dico de label pour l'instance\n",
    "    def learn_one(self, x: dict, y: dict):\n",
    "        feature_tensor = torch.from_numpy(x).double().to(device)\n",
    "        label_tensor = torch.from_numpy(y).double().to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        pred_tensor = self.model.forward(feature_tensor)\n",
    "        lossVal = self.loss(pred_tensor, label_tensor)\n",
    "        lossVal.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def predict_one(self, features: dict): #Renvoit un prédiction pour chaque label comprise entre 0 et 1.\n",
    "        feature_tensor = torch.from_numpy(features).double().to(device)\n",
    "        pred_tensor = self.model.forward(feature_tensor)\n",
    "        pred_detached = pred_tensor.cpu().clone().detach()\n",
    "        pred = pred_detached.numpy() #Convertit en numpy\n",
    "        return pred.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage sur le flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du jeu de données, définition des targets dans le modèle, choix de la métrique, définition du modèle\n",
    "dataset = \"data/datasets/20NG.arff\"\n",
    "target = [\n",
    "        \"y0\",\n",
    "        \"y1\",\n",
    "        \"y2\",\n",
    "        \"y3\",\n",
    "        \"y4\",\n",
    "        \"y5\",\n",
    "        \"y6\",\n",
    "        \"y7\",\n",
    "        \"y8\",\n",
    "        \"y9\",\n",
    "        \"y10\",\n",
    "        \"y11\",\n",
    "        \"y12\",\n",
    "        \"y13\",\n",
    "        \"y14\",\n",
    "        \"y15\",\n",
    "        \"y16\",\n",
    "        \"y17\",\n",
    "        \"y18\",\n",
    "        \"y19\",\n",
    "    ]\n",
    "metric = metrics.multioutput.SampleAverage(metrics.Jaccard()) #Métrique pour la perf du modèle\n",
    "metric_over_time = []\n",
    "i = 0\n",
    "model = NN(learning_rate=0.001,hidden_sizes=300)\n",
    "\n",
    "# Définition du tracker pour code carbon, qui permet de suivre et d'estimer les émissions de CO2 liées à l'éxec du code\n",
    "# tracker = OfflineEmissionsTracker(\n",
    "#         tracking_mode=\"process\",\n",
    "#         output_file=\"output/consommation.csv\",\n",
    "#         country_iso_code=\"FRA\")\n",
    "# tracker.start()\n",
    "\n",
    "#Début de l'évaluation sur le flux\n",
    "for x, y in stream.iter_arff(dataset, target, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "    # Répartition des attributs sur un array numpy\n",
    "    new_x = np.zeros(1006)\n",
    "    for key, value in x.items():\n",
    "        x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "        k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "        new_x[k] = value\n",
    "    # Prédiction\n",
    "    y_pred = model.predict_one(new_x)\n",
    "    n = 0\n",
    "    # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "    dict_y_pred = dict()\n",
    "    for j in y_pred :\n",
    "        if j == None : j = False\n",
    "        elif j >= 0.5 : j = True\n",
    "        else : j = False\n",
    "        dict_y_pred['y{}'.format(n)] = j\n",
    "        n+=1\n",
    "    # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "    for k, v in y.items():\n",
    "        y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "    # Répartition des labels de y sur un array numpy\n",
    "    new_y = np.zeros(20)\n",
    "    for key, value in y.items():\n",
    "        k = int(re.findall(r'\\d+',key)[0])\n",
    "        new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "    # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "    metric.update(y, dict_y_pred)\n",
    "    print(\"{} | \".format(i) + \"Jaccard accuracy : {}\".format(metric.get()))\n",
    "    metric_over_time.append(metric.get())\n",
    "    i+=1\n",
    "    # Apprentissage sur les array numpy de x et y\n",
    "    model.learn_one(new_x, new_y)\n",
    "# Fin du tracker de code carbone\n",
    "# tracker.stop()\n",
    "\n",
    "# Enregistrement des résultats sur un fichier json\n",
    "with open(\"output/results.json\", \"w\") as outfile:\n",
    "    json.dump(metric_over_time, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(metric_over_time, label='Indice Jaccard au cours du temps')\n",
    "plt.xlabel('Instances')\n",
    "plt.ylabel('Indice Jaccard')\n",
    "plt.title('Evolution de l''indice Jaccard au cours du temps')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_iteration(dataset_path, target_labels, epochs=1):\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Initialiser le modèle avec la taille de couche cachée actuelle\n",
    "        model = NN(learning_rate=0.001,feature_size=1006,hidden_sizes=300, label_size=20)\n",
    "        metric_over_time = []\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "\n",
    "        # Boucle sur le dataset\n",
    "        for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "            # Répartition des attributs sur un array numpy\n",
    "            new_x = np.zeros(1006)\n",
    "            for key, value in x.items():\n",
    "                x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                new_x[k] = value\n",
    "            # Prédiction\n",
    "            y_pred = model.predict_one(new_x)\n",
    "            n = 0\n",
    "            # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "            dict_y_pred = dict()\n",
    "            for j in y_pred :\n",
    "                if j == None : j = False\n",
    "                elif j >= 0.5 : j = True\n",
    "                else : j = False\n",
    "                dict_y_pred['y{}'.format(n)] = j\n",
    "                n+=1\n",
    "            # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "            for k, v in y.items():\n",
    "                y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "            # Répartition des labels de y sur un array numpy\n",
    "            new_y = np.zeros(20)\n",
    "            for key, value in y.items():\n",
    "                k = int(re.findall(r'\\d+',key)[0])\n",
    "                new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "                \n",
    "            # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "            metric.update(y, dict_y_pred)\n",
    "            metric_over_time.append(metric.get())\n",
    "            # Apprentissage sur les array numpy de x et y\n",
    "            model.learn_one(new_x, new_y)\n",
    "\n",
    "        results.append(metric.get())\n",
    "        print(\"Epoch\",epoch,\"Indice Jaccard\",results[epoch])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "\n",
    "target_labels = [\n",
    "        \"y0\",\n",
    "        \"y1\",\n",
    "        \"y2\",\n",
    "        \"y3\",\n",
    "        \"y4\",\n",
    "        \"y5\",\n",
    "        \"y6\",\n",
    "        \"y7\",\n",
    "        \"y8\",\n",
    "        \"y9\",\n",
    "        \"y10\",\n",
    "        \"y11\",\n",
    "        \"y12\",\n",
    "        \"y13\",\n",
    "        \"y14\",\n",
    "        \"y15\",\n",
    "        \"y16\",\n",
    "        \"y17\",\n",
    "        \"y18\",\n",
    "        \"y19\",\n",
    "    ]\n",
    "\n",
    "results = NN_iteration(dataset_path, target_labels,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moy = np.mean(results)\n",
    "print(moy)\n",
    "sd = np.std(results)\n",
    "print(sd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation des méta-paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méta-paramètre : Nombre de couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def optimize_neuron_count(hidden_sizes, dataset_path, target_labels, epochs=1):\n",
    "    results = {}\n",
    "    time_results = {}\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        print(f\"Testing hidden layer size: {hidden_size}\")\n",
    "        # Initialiser le modèle avec la taille de couche cachée actuelle\n",
    "        model = NN(learning_rate=0.01,feature_size=1006,hidden_sizes=hidden_size, label_size=20)\n",
    "        metric_over_time = []\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "\n",
    "        # Boucle sur le dataset\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()  # Début de mesure du temps pour l'epoch\n",
    "            for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "                # Répartition des attributs sur un array numpy\n",
    "                new_x = np.zeros(1006)\n",
    "                for key, value in x.items():\n",
    "                    x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                    k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                    new_x[k] = value\n",
    "                # Prédiction\n",
    "                y_pred = model.predict_one(new_x)\n",
    "                n = 0\n",
    "                # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "                dict_y_pred = dict()\n",
    "                for j in y_pred :\n",
    "                    if j == None : j = False\n",
    "                    elif j >= 0.5 : j = True\n",
    "                    else : j = False\n",
    "                    dict_y_pred['y{}'.format(n)] = j\n",
    "                    n+=1\n",
    "                # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "                for k, v in y.items():\n",
    "                    y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "                # Répartition des labels de y sur un array numpy\n",
    "                new_y = np.zeros(20)\n",
    "                for key, value in y.items():\n",
    "                    k = int(re.findall(r'\\d+',key)[0])\n",
    "                    new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "                    \n",
    "                # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "                metric.update(y, dict_y_pred)\n",
    "                metric_over_time.append(metric.get())\n",
    "                # Apprentissage sur les array numpy de x et y\n",
    "                model.learn_one(new_x, new_y)\n",
    "\n",
    "            end_time = time.time()  # Fin de mesure du temps pour l'epoch\n",
    "            epoch_duration = end_time - start_time\n",
    "            print(f\"Epoch {epoch+1}, Time: {epoch_duration}\")\n",
    "            metric_over_time.append(metric.get())\n",
    "            print(f\"Epoch {epoch+1}, Jaccard accuracy: {metric.get()}\")\n",
    "\n",
    "        results[hidden_size] = metric.get()\n",
    "        time_results[hidden_size] = epoch_duration\n",
    "        print(f\"Hidden Size {hidden_size}, Jaccard Index: {results[hidden_size]}, Time : {time_results[hidden_size]}\")\n",
    "\n",
    "    return results,time_results\n",
    "\n",
    "# Liste des tailles de couches cachées à tester\n",
    "hidden_sizes = [100, 300, 600,900]\n",
    "\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "\n",
    "target_labels = [\n",
    "        \"y0\",\n",
    "        \"y1\",\n",
    "        \"y2\",\n",
    "        \"y3\",\n",
    "        \"y4\",\n",
    "        \"y5\",\n",
    "        \"y6\",\n",
    "        \"y7\",\n",
    "        \"y8\",\n",
    "        \"y9\",\n",
    "        \"y10\",\n",
    "        \"y11\",\n",
    "        \"y12\",\n",
    "        \"y13\",\n",
    "        \"y14\",\n",
    "        \"y15\",\n",
    "        \"y16\",\n",
    "        \"y17\",\n",
    "        \"y18\",\n",
    "        \"y19\",\n",
    "    ]\n",
    "\n",
    "neuron_results,time_results = optimize_neuron_count(hidden_sizes, dataset_path, target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de graphiques pour la performance et la vitesse\n",
    "sizes = list(neuron_results.keys())\n",
    "jaccard_scores = list(neuron_results.values())\n",
    "times = list(time_results.values())\n",
    "\n",
    "print(times)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Graphique pour l'indice de Jaccard\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sizes, jaccard_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Taille des couches cachées')\n",
    "plt.ylabel('Indice de Jaccard moyen')\n",
    "plt.title('Impact de la Taille des Couches Cachées sur l\\'Indice de Jaccard')\n",
    "plt.grid(True)\n",
    "\n",
    "# Graphique pour le temps d'exécution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sizes, times, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Taille des couches cachées')\n",
    "plt.ylabel('Temps moyen (secondes)')\n",
    "plt.title('Impact de la Taille des Couches Cachées sur le Temps d\\'Exécution')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méta-paramètre : Taux d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def optimize_learning_rate(learning_rates, dataset_path, target_labels, epochs=1):\n",
    "    results = {}\n",
    "    time_results = {}\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Testing learning rate: {lr}\")\n",
    "        # Initialiser le modèle avec le taux d'apprentissage actuel\n",
    "        model = NN(learning_rate=lr,hidden_sizes=300)\n",
    "        metric_over_time = []\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "\n",
    "        # Boucle sur le dataset\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "                # Répartition des attributs sur un array numpy\n",
    "                new_x = np.zeros(1006)\n",
    "                for key, value in x.items():\n",
    "                    x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                    k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                    new_x[k] = value\n",
    "                # Prédiction\n",
    "                y_pred = model.predict_one(new_x)\n",
    "                n = 0\n",
    "                # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "                dict_y_pred = dict()\n",
    "                for j in y_pred :\n",
    "                    if j == None : j = False\n",
    "                    elif j >= 0.5 : j = True\n",
    "                    else : j = False\n",
    "                    dict_y_pred['y{}'.format(n)] = j\n",
    "                    n+=1\n",
    "                # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "                for k, v in y.items():\n",
    "                    y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "                # Répartition des labels de y sur un array numpy\n",
    "                new_y = np.zeros(20)\n",
    "                for key, value in y.items():\n",
    "                    k = int(re.findall(r'\\d+',key)[0])\n",
    "                    new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "                    \n",
    "                # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "                metric.update(y, dict_y_pred)\n",
    "                metric_over_time.append(metric.get())\n",
    "                # Apprentissage sur les array numpy de x et y\n",
    "                model.learn_one(new_x, new_y)\n",
    "\n",
    "            end_time = time.time()  # Fin de mesure du temps pour l'epoch\n",
    "            epoch_duration = end_time - start_time\n",
    "            print(f\"Epoch {epoch+1}, Time: {epoch_duration}\")\n",
    "            metric_over_time.append(metric.get())\n",
    "            print(f\"Epoch {epoch+1}, Jaccard accuracy: {metric.get()}\")\n",
    "\n",
    "        results[lr] = metric.get()\n",
    "        time_results[lr] = epoch_duration\n",
    "        print(f\"Learning Rates {lr}, Jaccard Index: {results[lr]}, Time : {time_results[lr]}\")\n",
    "\n",
    "    return results,time_results\n",
    "\n",
    "# Liste des taux d'apprentissage à tester\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.02, 0.05]\n",
    "\n",
    "# Chemin vers le dataset et cibles\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "\n",
    "# Exécution de l'optimisation\n",
    "lr_results,lr_time_results= optimize_learning_rate(learning_rates, dataset_path, target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de graphiques pour la performance et la vitesse\n",
    "lr = list(lr_results.keys())\n",
    "jaccard_scores = list(lr_results.values())\n",
    "times = list(lr_time_results.values())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Graphique pour l'indice de Jaccard\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(learning_rates, jaccard_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Taux d\\'apprentissage')\n",
    "plt.ylabel('Indice de Jaccard moyen')\n",
    "plt.title('Impact du Taux d\\'Apprentissage sur l\\'Indice de Jaccard')\n",
    "plt.grid(True)\n",
    "\n",
    "# Graphique pour le temps d'exécution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(learning_rates, times, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Taux d\\'apprentissage')\n",
    "plt.ylabel('Temps moyen (secondes)')\n",
    "plt.title('Impact du Taux d\\'Apprentissage sur la Vitesse du Modèle')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méthode Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, data):\n",
    "        self.buffer.append(data)\n",
    "    \n",
    "    def sample(self, batch_size): # Cette méthode extrait un échantillon aléatoire de données du buffer.\n",
    "        return random.sample(self.buffer, min(len(self.buffer), batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class NNWithReplay(nn.Module):\n",
    "    def __init__(self, learning_rate=0.01, feature_size=1006, hidden_sizes=200, label_size=20, buffer_capacity=1000, replay_batch_size=20, replay_frequency=100):\n",
    "        super().__init__()\n",
    "        self.model = Architecture(feature_size, hidden_sizes, label_size)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.replay_frequency = replay_frequency\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.iteration_count = 0\n",
    "\n",
    "    def learn_one(self, x, y):\n",
    "        feature_tensor = torch.from_numpy(x).float().to(self.device)\n",
    "        label_tensor = torch.from_numpy(y).float().to(self.device)\n",
    "\n",
    "        # Ajouter au buffer de replay\n",
    "        self.replay_buffer.push((feature_tensor, label_tensor))\n",
    "\n",
    "        # Apprentissage avec les données en temps réel\n",
    "        self._train_step(feature_tensor, label_tensor)\n",
    "\n",
    "        # Apprentissage avec les données de replay\n",
    "        if self.iteration_count % self.replay_frequency == 0 and len(self.replay_buffer.buffer) >= self.replay_batch_size: #Si le buffer a accumulé assez de donnée + fréquence alors apprentissage\n",
    "            replay_samples = self.replay_buffer.sample(self.replay_batch_size)\n",
    "            for sample_x, sample_y in replay_samples:\n",
    "                self._train_step(sample_x, sample_y)\n",
    "        \n",
    "        # Incrémenter le compteur d'itérations\n",
    "        self.iteration_count += 1\n",
    "\n",
    "    def _train_step(self, feature_tensor, label_tensor):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred_tensor = self.model(feature_tensor)\n",
    "        loss = self.loss(pred_tensor, label_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict_one(self, features):\n",
    "        feature_tensor = torch.from_numpy(features).float().to(self.device)\n",
    "        with torch.no_grad():\n",
    "            pred_tensor = self.model(feature_tensor)\n",
    "        return pred_tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du jeu de données, définition des targets dans le modèle, choix de la métrique, définition du modèle\n",
    "dataset = \"data/datasets/20NG.arff\"\n",
    "target = [\n",
    "        \"y0\",\n",
    "        \"y1\",\n",
    "        \"y2\",\n",
    "        \"y3\",\n",
    "        \"y4\",\n",
    "        \"y5\",\n",
    "        \"y6\",\n",
    "        \"y7\",\n",
    "        \"y8\",\n",
    "        \"y9\",\n",
    "        \"y10\",\n",
    "        \"y11\",\n",
    "        \"y12\",\n",
    "        \"y13\",\n",
    "        \"y14\",\n",
    "        \"y15\",\n",
    "        \"y16\",\n",
    "        \"y17\",\n",
    "        \"y18\",\n",
    "        \"y19\",\n",
    "    ]\n",
    "metric = metrics.multioutput.SampleAverage(metrics.Jaccard()) #Métrique pour la perf du modèle\n",
    "metric_over_time = []\n",
    "i = 0\n",
    "model = NNWithReplay(learning_rate=0.001, feature_size=1006, hidden_sizes=300, label_size=20,buffer_capacity=500,replay_batch_size=20,replay_frequency=100)\n",
    "\n",
    "#Début de l'évaluation sur le flux\n",
    "for x, y in stream.iter_arff(dataset, target, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "    # Répartition des attributs sur un array numpy\n",
    "    new_x = np.zeros(1006)\n",
    "    for key, value in x.items():\n",
    "        x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "        k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "        new_x[k] = value\n",
    "    # Prédiction\n",
    "    y_pred = model.predict_one(new_x)\n",
    "    n = 0\n",
    "    # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "    dict_y_pred = dict()\n",
    "    for j in y_pred :\n",
    "        if j == None : j = False\n",
    "        elif j >= 0.5 : j = True\n",
    "        else : j = False\n",
    "        dict_y_pred['y{}'.format(n)] = j\n",
    "        n+=1\n",
    "    # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "    for k, v in y.items():\n",
    "        y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "    # Répartition des labels de y sur un array numpy\n",
    "    new_y = np.zeros(20)\n",
    "    for key, value in y.items():\n",
    "        k = int(re.findall(r'\\d+',key)[0])\n",
    "        new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "    # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "    metric.update(y, dict_y_pred)\n",
    "    print(\"{} | \".format(i) + \"Jaccard accuracy : {}\".format(metric.get()))\n",
    "    metric_over_time.append(metric.get())\n",
    "    i+=1\n",
    "    # Apprentissage sur les array numpy de x et y\n",
    "    model.learn_one(new_x, new_y)\n",
    "# Fin du tracker de code carbone\n",
    "# tracker.stop()\n",
    "\n",
    "# Enregistrement des résultats sur un fichier json\n",
    "with open(\"output/results.json\", \"w\") as outfile:\n",
    "    json.dump(metric_over_time, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_iteration(dataset_path, target_labels, epochs=1, learning_rate=0.001, hidden_sizes=300,feature_size=1006,buffer_size=500, label_size=20,replay_batch_size=20,replay_frequency=10):\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Initialiser le modèle avec la taille du buffer spécifiée\n",
    "        model = NNWithReplay(learning_rate=learning_rate, feature_size=feature_size, hidden_sizes=hidden_sizes,\n",
    "                             label_size=label_size, buffer_capacity=buffer_size, replay_batch_size=replay_batch_size, replay_frequency=replay_frequency)\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "        metric_over_time = []\n",
    "        \n",
    "    \n",
    "        for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "            # Répartition des attributs sur un array numpy\n",
    "            new_x = np.zeros(1006)\n",
    "            for key, value in x.items():\n",
    "                x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                new_x[k] = value\n",
    "            # Prédiction\n",
    "            y_pred = model.predict_one(new_x)\n",
    "            n = 0\n",
    "            # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "            dict_y_pred = dict()\n",
    "            for j in y_pred :\n",
    "                if j == None : j = False\n",
    "                elif j >= 0.5 : j = True\n",
    "                else : j = False\n",
    "                dict_y_pred['y{}'.format(n)] = j\n",
    "                n+=1\n",
    "            # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "            for k, v in y.items():\n",
    "                y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "            # Répartition des labels de y sur un array numpy\n",
    "            new_y = np.zeros(20)\n",
    "            for key, value in y.items():\n",
    "                k = int(re.findall(r'\\d+',key)[0])\n",
    "                new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "                \n",
    "            # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "            metric.update(y, dict_y_pred)\n",
    "            metric_over_time.append(metric.get())\n",
    "            # Apprentissage sur les array numpy de x et y\n",
    "            model.learn_one(new_x, new_y)\n",
    "\n",
    "        results.append(metric.get())\n",
    "        print(f\"Epoch: {epoch}, Jaccard Index: {metric.get()}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Chemin vers le dataset et cibles\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "\n",
    "# Exécution de l'optimisation\n",
    "buffer_capacity_results = replay_iteration(dataset_path, target_labels,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moy = np.mean(buffer_capacity_results)\n",
    "print(moy)\n",
    "std = np.std(buffer_capacity_results)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optmisation méta-paramètre de la Méthode Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méta-paramètre : Buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_buffer_capacity(buffer_sizes, dataset_path, target_labels, epochs=1, learning_rate=0.01, hidden_sizes=300,feature_size=1006, label_size=20,replay_batch_size=20,replay_frequency=1000):\n",
    "    results = {}\n",
    "    time_results = {}\n",
    "\n",
    "    for buffer_size in buffer_sizes:\n",
    "        print(f\"Testing buffer capacity: {buffer_size}\")\n",
    "        # Initialiser le modèle avec la taille du buffer spécifiée\n",
    "        model = NNWithReplay(learning_rate=learning_rate, feature_size=feature_size, hidden_sizes=hidden_sizes,\n",
    "                             label_size=label_size, buffer_capacity=buffer_size, replay_batch_size=replay_batch_size, replay_frequency=replay_frequency)\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "        metric_over_time = []\n",
    "        \n",
    "        # Boucle sur le dataset\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "                # Répartition des attributs sur un array numpy\n",
    "                new_x = np.zeros(1006)\n",
    "                for key, value in x.items():\n",
    "                    x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                    k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                    new_x[k] = value\n",
    "                # Prédiction\n",
    "                y_pred = model.predict_one(new_x)\n",
    "                n = 0\n",
    "                # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "                dict_y_pred = dict()\n",
    "                for j in y_pred :\n",
    "                    if j == None : j = False\n",
    "                    elif j >= 0.5 : j = True\n",
    "                    else : j = False\n",
    "                    dict_y_pred['y{}'.format(n)] = j\n",
    "                    n+=1\n",
    "                # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "                for k, v in y.items():\n",
    "                    y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "                # Répartition des labels de y sur un array numpy\n",
    "                new_y = np.zeros(20)\n",
    "                for key, value in y.items():\n",
    "                    k = int(re.findall(r'\\d+',key)[0])\n",
    "                    new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "                    \n",
    "                # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "                metric.update(y, dict_y_pred)\n",
    "                metric_over_time.append(metric.get())\n",
    "                # Apprentissage sur les array numpy de x et y\n",
    "                model.learn_one(new_x, new_y)\n",
    "\n",
    "            end_time = time.time()  # Fin de mesure du temps pour l'epoch\n",
    "            epoch_duration = end_time - start_time\n",
    "            metric_over_time.append(metric.get())\n",
    "\n",
    "        results[buffer_size] = metric.get()\n",
    "        time_results[buffer_size] = epoch_duration\n",
    "        print(f\"Buffer Size {buffer_size}, Jaccard Index: {results[buffer_size]}, Time : {time_results[buffer_size]}\")\n",
    "\n",
    "    return results,time_results\n",
    "\n",
    "\n",
    "# Liste des tailles de mémoire à tester\n",
    "buffer_sizes = [100, 1000, 2500, 5000]\n",
    "\n",
    "# Chemin vers le dataset et cibles\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "\n",
    "# Exécution de l'optimisation\n",
    "buffer_capacity_results,buffer_time_results = optimize_buffer_capacity(buffer_sizes, dataset_path, target_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de graphiques pour la performance et la vitesse\n",
    "buffer_sizes = list(buffer_capacity_results.keys())\n",
    "jaccard_scores = list(buffer_capacity_results.values())\n",
    "times = list(buffer_time_results.values())\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Graphique pour l'indice de Jaccard\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(buffer_sizes, jaccard_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Capacité du Buffer')\n",
    "plt.ylabel('Indice de Jaccard')\n",
    "plt.title('Impact de la Capacité du Buffer sur l\\'Indice de Jaccard')\n",
    "plt.grid(True)\n",
    "\n",
    "# Graphique pour le temps d'exécution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(buffer_sizes, times, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Capacité du Buffer')\n",
    "plt.ylabel('Temps moyen (secondes)')\n",
    "plt.title('Impact de la Capacité des Buffer sur le Temps d\\'Exécution')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méta-paramètre : Replay_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def optimize_replay_frequency(frequencies, dataset_path, target_labels, epochs=1, learning_rate=0.01, hidden_sizes=300, label_size=20, buffer_capacity=1000,replay_batch_size=20):\n",
    "    results = {}\n",
    "    time_results = {}\n",
    "\n",
    "    for frequency in frequencies:\n",
    "        print(f\"Testing replay frequency: {frequency}\")\n",
    "        # Initialiser le modèle avec la fréquence de replay spécifiée\n",
    "        model = NNWithReplay(learning_rate=learning_rate, feature_size=1006, hidden_sizes=hidden_sizes,\n",
    "                             label_size=label_size, buffer_capacity=buffer_capacity, replay_batch_size=replay_batch_size, replay_frequency=frequency)\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "        metric_over_time = []\n",
    "\n",
    "        # Boucle d'entraînement sur le dataset\n",
    "        # Boucle sur le dataset\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "                # Répartition des attributs sur un array numpy\n",
    "                new_x = np.zeros(1006)\n",
    "                for key, value in x.items():\n",
    "                    x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                    k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                    new_x[k] = value\n",
    "                # Prédiction\n",
    "                y_pred = model.predict_one(new_x)\n",
    "                n = 0\n",
    "                # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "                dict_y_pred = dict()\n",
    "                for j in y_pred :\n",
    "                    if j == None : j = False\n",
    "                    elif j >= 0.5 : j = True\n",
    "                    else : j = False\n",
    "                    dict_y_pred['y{}'.format(n)] = j\n",
    "                    n+=1\n",
    "                # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "                for k, v in y.items():\n",
    "                    y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "                # Répartition des labels de y sur un array numpy\n",
    "                new_y = np.zeros(20)\n",
    "                for key, value in y.items():\n",
    "                    k = int(re.findall(r'\\d+',key)[0])\n",
    "                    new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "                    \n",
    "                # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "                metric.update(y, dict_y_pred)\n",
    "                metric_over_time.append(metric.get())\n",
    "                # Apprentissage sur les array numpy de x et y\n",
    "                model.learn_one(new_x, new_y)\n",
    "\n",
    "            end_time = time.time()  # Fin de mesure du temps pour l'epoch\n",
    "            epoch_duration = end_time - start_time\n",
    "            metric_over_time.append(metric.get())\n",
    "\n",
    "        results[frequency] = metric.get()\n",
    "        time_results[frequency] = epoch_duration\n",
    "        print(f\"Replay frequency {frequency}, Jaccard Index: {results[frequency]}, Time : {time_results[frequency]}\")\n",
    "\n",
    "    return results,time_results\n",
    "\n",
    "# Liste des fréquences de replay à tester\n",
    "frequencies = [10,100,500,1000]\n",
    "\n",
    "# Chemin vers le dataset et cibles\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "\n",
    "# Exécution de l'optimisation\n",
    "frequency_results,frequency_time_results = optimize_replay_frequency(frequencies, dataset_path, target_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Préparation des données pour le graphique\n",
    "frequencies = list(frequency_results.keys())\n",
    "jaccard_scores = list(frequency_results.values())\n",
    "times = list(frequency_time_results.values())\n",
    "\n",
    "print(frequency_time_results)\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Graphique pour l'indice de Jaccard\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(frequencies, jaccard_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Fréquence de Replay')\n",
    "plt.ylabel('Indice de Jaccard')\n",
    "plt.title('Impact de la Fréquence du Replay sur l\\'Indice de Jaccard')\n",
    "plt.grid(True)\n",
    "\n",
    "# Graphique pour le temps d'exécution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(frequencies, times, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Fréquence de Replay')\n",
    "plt.ylabel('Temps moyen (secondes)')\n",
    "plt.title('Impact de la Fréquence du Replay sur le Temps d\\'Exécution')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Méta_paramètre : replay_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def optimize_replay_batch_size(batch_sizes, dataset_path, target_labels, epochs=1,learning_rate=0.01, hidden_sizes=300,feature_size=1006, label_size=20,buffer_sizes=1000,replay_frequency=1000):\n",
    "    results = {}\n",
    "    time_results = {}\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"Testing replay batch size: {batch_size}\")\n",
    "        # Initialiser le modèle avec la taille de batch de replay actuelle\n",
    "        model = NNWithReplay(learning_rate=learning_rate, feature_size=feature_size, hidden_sizes=hidden_sizes, label_size=label_size,\n",
    "                             buffer_capacity=buffer_sizes, replay_batch_size=batch_size, replay_frequency=replay_frequency)\n",
    "        metric_over_time = []\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "        \n",
    "        # Boucle sur le dataset\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()\n",
    "            for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "                # Répartition des attributs sur un array numpy\n",
    "                new_x = np.zeros(1006)\n",
    "                for key, value in x.items():\n",
    "                    x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                    k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                    new_x[k] = value\n",
    "                # Prédiction\n",
    "                y_pred = model.predict_one(new_x)\n",
    "                n = 0\n",
    "                # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "                dict_y_pred = dict()\n",
    "                for j in y_pred :\n",
    "                    if j == None : j = False\n",
    "                    elif j >= 0.5 : j = True\n",
    "                    else : j = False\n",
    "                    dict_y_pred['y{}'.format(n)] = j\n",
    "                    n+=1\n",
    "                # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "                for k, v in y.items():\n",
    "                    y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "                # Répartition des labels de y sur un array numpy\n",
    "                new_y = np.zeros(20)\n",
    "                for key, value in y.items():\n",
    "                    k = int(re.findall(r'\\d+',key)[0])\n",
    "                    new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "                    \n",
    "                # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "                metric.update(y, dict_y_pred)\n",
    "                metric_over_time.append(metric.get())\n",
    "                # Apprentissage sur les array numpy de x et y\n",
    "                model.learn_one(new_x, new_y)\n",
    "\n",
    "            end_time = time.time()  # Fin de mesure du temps pour l'epoch\n",
    "            epoch_duration = end_time - start_time\n",
    "            metric_over_time.append(metric.get())\n",
    "\n",
    "        results[batch_size] = metric.get()\n",
    "        time_results[batch_size] = epoch_duration\n",
    "        print(f\"Batch Size {batch_size}, Jaccard Index: {results[batch_size]}, Time : {time_results[batch_size]}\")\n",
    "\n",
    "    return results,time_results\n",
    "\n",
    "batch_sizes = [10,100,500,1000]\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "batch_size_results,batch_time_results = optimize_replay_batch_size(batch_sizes, dataset_path, target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraction des données pour le graphique\n",
    "sizes = list(batch_size_results.keys())\n",
    "jaccard_scores = list(batch_size_results.values())\n",
    "times = list(batch_time_results.values())\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Graphique pour l'indice de Jaccard\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_sizes, jaccard_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Taille du Batch de Replay')\n",
    "plt.ylabel('Indice de Jaccard moyen')\n",
    "plt.title('Impact de la Taille du Batch de Replay sur l\\'Indice de Jaccard')\n",
    "plt.grid(True)\n",
    "\n",
    "# Graphique pour le temps d'exécution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(batch_sizes, times, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Taille du Batch de Replay')\n",
    "plt.ylabel('Temps moyen (secondes)')\n",
    "plt.title('Impact du Batch de Replay sur le Temps d\\'Exécution')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN avec LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)# Initialisation du LSTM\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: x doit être de la forme (batch, sequence_length, features)\n",
    "        lstm_out, _ = self.lstm(x) # Traitement de la séquence par le LSTM.\n",
    "        # Extraction de la sortie du dernier pas de temps de chaque séquence pour la classification.\n",
    "        last_time_step = lstm_out[:, -1, :]\n",
    "        output = self.linear(last_time_step) #Application de la couche linéaire\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du modèle\n",
    "class LSTMForContinualLearning:\n",
    "    def __init__(self, learning_rate=0.01, feature_size=1006, hidden_sizes=200, label_size=20,sequence_length=5):\n",
    "        self.model = LSTMModel(feature_size, hidden_sizes, label_size) #Initialisation du modèle LSTM \n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        self.buffer = deque([], maxlen=sequence_length)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.sequence_length = sequence_length\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self, features, labels):\n",
    "        self.buffer.append(features)\n",
    "        if len(self.buffer) < self.sequence_length:\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "            self.buffer.popleft()  # Supprimez le premier élément pour permettre le chevauchement\n",
    "        labels = torch.tensor([labels], dtype=torch.float32).to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(features)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict_one(self, features):\n",
    "        # Préparation des données pour la prédiction\n",
    "        self.buffer.append(features)\n",
    "        if len(self.buffer) < self.sequence_length:\n",
    "            # Si le buffer n'est pas encore plein, utilisez simplement les données disponibles\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            # Si le buffer est plein, faites la prédiction et gérez le chevauchement\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "            self.buffer.popleft()  # Supprimez le premier élément pour permettre le chevauchement\n",
    "        with torch.no_grad():\n",
    "            pred_tensor = self.model(features)\n",
    "        return pred_tensor.cpu().numpy()[0] #Retourne la prédiction après suppression de la dimension de batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du jeu de données, définition des targets dans le modèle, choix de la métrique, définition du modèle\n",
    "dataset = \"data/datasets/20NG.arff\"\n",
    "target = [\n",
    "        \"y0\",\n",
    "        \"y1\",\n",
    "        \"y2\",\n",
    "        \"y3\",\n",
    "        \"y4\",\n",
    "        \"y5\",\n",
    "        \"y6\",\n",
    "        \"y7\",\n",
    "        \"y8\",\n",
    "        \"y9\",\n",
    "        \"y10\",\n",
    "        \"y11\",\n",
    "        \"y12\",\n",
    "        \"y13\",\n",
    "        \"y14\",\n",
    "        \"y15\",\n",
    "        \"y16\",\n",
    "        \"y17\",\n",
    "        \"y18\",\n",
    "        \"y19\",\n",
    "    ]\n",
    "metric = metrics.multioutput.SampleAverage(metrics.Jaccard()) #Métrique pour la perf du modèle\n",
    "metric_over_time = []\n",
    "i = 0\n",
    "model = LSTMForContinualLearning(learning_rate=0.001, feature_size=1006, hidden_sizes=300, label_size=20,sequence_length=1)\n",
    "\n",
    "#Début de l'évaluation sur le flux\n",
    "for x, y in stream.iter_arff(dataset, target, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "    # Répartition des attributs sur un array numpy\n",
    "    new_x = np.zeros(1006)\n",
    "    for key, value in x.items():\n",
    "        x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "        k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "        new_x[k] = value\n",
    "    # Prédiction\n",
    "    y_pred = model.predict_one(new_x)\n",
    "    n = 0\n",
    "    # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "    dict_y_pred = dict()\n",
    "    for j in y_pred :\n",
    "        if j == None : j = False\n",
    "        elif j >= 0.5 : j = True\n",
    "        else : j = False\n",
    "        dict_y_pred['y{}'.format(n)] = j\n",
    "        n+=1\n",
    "    # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "    for k, v in y.items():\n",
    "        y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "    # Répartition des labels de y sur un array numpy\n",
    "    new_y = np.zeros(20)\n",
    "    for key, value in y.items():\n",
    "        k = int(re.findall(r'\\d+',key)[0])\n",
    "        new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "    # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "    metric.update(y, dict_y_pred)\n",
    "    print(\"{} | \".format(i) + \"Jaccard accuracy : {}\".format(metric.get()))\n",
    "    metric_over_time.append(metric.get())\n",
    "    i+=1\n",
    "    # Apprentissage sur les array numpy de x et y\n",
    "    model.train(new_x, new_y)\n",
    "# Fin du tracker de code carbone\n",
    "# tracker.stop()\n",
    "\n",
    "# Enregistrement des résultats sur un fichier json\n",
    "with open(\"output/results.json\", \"w\") as outfile:\n",
    "    json.dump(metric_over_time, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_iteration(dataset_path, target_labels,epochs):\n",
    "    results = []\n",
    "    for epoch in range(epochs):\n",
    "        model = LSTMForContinualLearning(learning_rate=0.001, feature_size=1006, hidden_sizes=300, label_size=20, sequence_length=1)\n",
    "        metric_over_time = []\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "        #Début de l'évaluation sur le flux\n",
    "        for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "            # Répartition des attributs sur un array numpy\n",
    "            new_x = np.zeros(1006)\n",
    "            for key, value in x.items():\n",
    "                x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                new_x[k] = value\n",
    "            # Prédiction\n",
    "            y_pred = model.predict_one(new_x)\n",
    "            n = 0\n",
    "            # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "            dict_y_pred = dict()\n",
    "            for j in y_pred :\n",
    "                if j == None : j = False\n",
    "                elif j >= 0.5 : j = True\n",
    "                else : j = False\n",
    "                dict_y_pred['y{}'.format(n)] = j\n",
    "                n+=1\n",
    "            # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "            for k, v in y.items():\n",
    "                y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "            # Répartition des labels de y sur un array numpy\n",
    "            new_y = np.zeros(20)\n",
    "            for key, value in y.items():\n",
    "                k = int(re.findall(r'\\d+',key)[0])\n",
    "                new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "            # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "            metric.update(y, dict_y_pred)\n",
    "            metric_over_time.append(metric.get())\n",
    "            # Apprentissage sur les array numpy de x et y\n",
    "            model.train(new_x, new_y)\n",
    "\n",
    "        results.append(metric.get())\n",
    "        print(f\"Epoch : {epoch}, Jaccard Index: {metric.get()}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Appel de la fonction\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "sequence_results =LSTM_iteration(dataset_path, target_labels,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moy = np.mean(sequence_results)\n",
    "print(moy)\n",
    "std = np.std(sequence_results)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Méta-paramètre : Séquence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def optimize_sequence_length(sequence_lengths, dataset_path, target_labels):\n",
    "    results = {}\n",
    "    time_results = {}\n",
    "    for length in sequence_lengths:\n",
    "        print(f\"Testing sequence length: {length}\")\n",
    "        model = LSTMForContinualLearning(learning_rate=0.01, feature_size=1006, hidden_sizes=300, label_size=20, sequence_length=length)\n",
    "        metric_over_time = []\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "        start_time = time.time()\n",
    "        #Début de l'évaluation sur le flux\n",
    "        for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "            # Répartition des attributs sur un array numpy\n",
    "            new_x = np.zeros(1006)\n",
    "            for key, value in x.items():\n",
    "                x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                new_x[k] = value\n",
    "            # Prédiction\n",
    "            y_pred = model.predict_one(new_x)\n",
    "            n = 0\n",
    "            # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "            dict_y_pred = dict()\n",
    "            for j in y_pred :\n",
    "                if j == None : j = False\n",
    "                elif j >= 0.5 : j = True\n",
    "                else : j = False\n",
    "                dict_y_pred['y{}'.format(n)] = j\n",
    "                n+=1\n",
    "            # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "            for k, v in y.items():\n",
    "                y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "            # Répartition des labels de y sur un array numpy\n",
    "            new_y = np.zeros(20)\n",
    "            for key, value in y.items():\n",
    "                k = int(re.findall(r'\\d+',key)[0])\n",
    "                new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "            # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "            metric.update(y, dict_y_pred)\n",
    "            metric_over_time.append(metric.get())\n",
    "            # Apprentissage sur les array numpy de x et y\n",
    "            model.train(new_x, new_y)\n",
    "            \n",
    "        end_time = time.time()  # Fin de mesure du temps pour l'epoch\n",
    "        epoch_duration = end_time - start_time\n",
    "        metric_over_time.append(metric.get())\n",
    "\n",
    "        results[length] = metric.get()\n",
    "        time_results[length] = epoch_duration\n",
    "        print(f\"Sequence Length {length}, Jaccard Index: {results[length]}, Time : {time_results[length]}\")\n",
    "\n",
    "    return results,time_results\n",
    "\n",
    "# Appel de la fonction\n",
    "sequence_lengths = [1,5,10,20]  # Différentes longueurs de séquences à tester\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "sequence_results,sequence_time_results =optimize_sequence_length(sequence_lengths, dataset_path, target_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraction des données pour le graphique\n",
    "sequence_lengths = list(sequence_results.keys())\n",
    "jaccard_scores = list(sequence_results.values())\n",
    "times = list(sequence_time_results.values())\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Graphique pour l'indice de Jaccard\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sequence_lengths, jaccard_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Taille de la fênetre')\n",
    "plt.ylabel('Indice de Jaccard moyen')\n",
    "plt.title('Impact de la fênetre temporel sur l\\'Indice de Jaccard')\n",
    "plt.grid(True)\n",
    "\n",
    "# Graphique pour le temps d'exécution\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sequence_lengths, times, marker='o', linestyle='-', color='r')\n",
    "plt.xlabel('Taille de la fênetre')\n",
    "plt.ylabel('Temps moyen (secondes)')\n",
    "plt.title('Impact du la fênetre temporel sur le Temps d\\'Exécution')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mémoire intégrée + Méthode Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchitectureWithMemory(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size,num_layers, batch_first=True)# Initialisation du LSTM\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: x doit être de la forme (batch, sequence_length, features)\n",
    "        lstm_out, _ = self.lstm(x) # Traitement de la séquence par le LSTM.\n",
    "        # Extraction de la sortie du dernier pas de temps de chaque séquence pour la classification.\n",
    "        last_time_step = lstm_out[:, -1, :]\n",
    "        output = self.linear(last_time_step) #Application de la couche linéaire\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNWithMemory(nn.Module):\n",
    "    def __init__(self, learning_rate=0.001, feature_size=1006, hidden_sizes=300, label_size=20, buffer_capacity=500, replay_batch_size=20, replay_frequency=10,sequence_length=1):\n",
    "        super().__init__()\n",
    "        # Initialisation du modèle avec mémoire, configuration du LSTM et des couches de sortie\n",
    "        self.model = ArchitectureWithMemory(feature_size, hidden_sizes, label_size)\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.buffer = deque([], maxlen=sequence_length)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        # Buffer de replay pour stocker et réutiliser les anciennes données\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.replay_frequency = replay_frequency\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.iteration_count =0\n",
    "\n",
    "    def learn_one(self, features, labels):\n",
    "        # Préparation des tensors pour les données et les labels\n",
    "        self.buffer.append(features)\n",
    "        if len(self.buffer) < self.sequence_length:\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "            self.buffer.popleft()  # Supprimez le premier élément pour permettre le chevauchement\n",
    "        labels = torch.tensor([labels], dtype=torch.float32).to(self.device)\n",
    "        # Ajout des données au buffer de replay\n",
    "        self.replay_buffer.push((features, labels))\n",
    "\n",
    "        # Apprentissage en temps réel avec les données courantes\n",
    "        self._train_step(features, labels)\n",
    "\n",
    "        # Apprentissage par replay lorsque le buffer atteint une certaine taille\n",
    "        if  self.iteration_count% self.replay_frequency == 0 and len(self.replay_buffer.buffer) >= self.replay_batch_size:\n",
    "            replay_samples = self.replay_buffer.sample(self.replay_batch_size)\n",
    "            for sample_x, sample_y in replay_samples:\n",
    "                self._train_step(sample_x, sample_y)\n",
    "\n",
    "        # Incrémenter le compteur d'itérations\n",
    "        self.iteration_count += 1\n",
    "\n",
    "    def _train_step(self, feature_tensor, label_tensor):\n",
    "        self.optimizer.zero_grad()\n",
    "        pred_tensor = self.model(feature_tensor)\n",
    "        loss = self.loss(pred_tensor, label_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def predict_one(self, features):\n",
    "        # Préparation des données pour la prédiction\n",
    "        self.buffer.append(features)\n",
    "        if len(self.buffer) < self.sequence_length:\n",
    "            # Si le buffer n'est pas encore plein, utilisez simplement les données disponibles\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "        else:\n",
    "            # Si le buffer est plein, faites la prédiction et gérez le chevauchement\n",
    "            features = torch.tensor([list(self.buffer)], dtype=torch.float32).to(self.device)\n",
    "            self.buffer.popleft()  # Supprimez le premier élément pour permettre le chevauchement\n",
    "        with torch.no_grad():\n",
    "            pred_tensor = self.model(features)\n",
    "        return pred_tensor.cpu().numpy()[0]  #Retourne la prédiction après suppression de la dimension de batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du jeu de données, définition des targets dans le modèle, choix de la métrique, définition du modèle\n",
    "dataset = \"data/datasets/20NG.arff\"\n",
    "target = [\n",
    "        \"y0\",\n",
    "        \"y1\",\n",
    "        \"y2\",\n",
    "        \"y3\",\n",
    "        \"y4\",\n",
    "        \"y5\",\n",
    "        \"y6\",\n",
    "        \"y7\",\n",
    "        \"y8\",\n",
    "        \"y9\",\n",
    "        \"y10\",\n",
    "        \"y11\",\n",
    "        \"y12\",\n",
    "        \"y13\",\n",
    "        \"y14\",\n",
    "        \"y15\",\n",
    "        \"y16\",\n",
    "        \"y17\",\n",
    "        \"y18\",\n",
    "        \"y19\",\n",
    "    ]\n",
    "metric = metrics.multioutput.SampleAverage(metrics.Jaccard()) #Métrique pour la perf du modèle\n",
    "metric_over_time = []\n",
    "i = 0\n",
    "model = NNWithMemory(learning_rate=0.001, feature_size=1006, hidden_sizes=300, label_size=20,sequence_length=1)\n",
    "\n",
    "#Début de l'évaluation sur le flux\n",
    "for x, y in stream.iter_arff(dataset, target, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "    # Répartition des attributs sur un array numpy\n",
    "    new_x = np.zeros(1006)\n",
    "    for key, value in x.items():\n",
    "        x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "        k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "        new_x[k] = value\n",
    "    # Prédiction\n",
    "    y_pred = model.predict_one(new_x)\n",
    "    n = 0\n",
    "    # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "    dict_y_pred = dict()\n",
    "    for j in y_pred :\n",
    "        if j == None : j = False\n",
    "        elif j >= 0.5 : j = True\n",
    "        else : j = False\n",
    "        dict_y_pred['y{}'.format(n)] = j\n",
    "        n+=1\n",
    "    # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "    for k, v in y.items():\n",
    "        y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "    # Répartition des labels de y sur un array numpy\n",
    "    new_y = np.zeros(20)\n",
    "    for key, value in y.items():\n",
    "        k = int(re.findall(r'\\d+',key)[0])\n",
    "        new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "    # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "    metric.update(y, dict_y_pred)\n",
    "    print(\"{} | \".format(i) + \"Jaccard accuracy : {}\".format(metric.get()))\n",
    "    metric_over_time.append(metric.get())\n",
    "    i+=1\n",
    "    # Apprentissage sur les array numpy de x et y\n",
    "    model.learn_one(new_x, new_y)\n",
    "# Fin du tracker de code carbone\n",
    "# tracker.stop()\n",
    "\n",
    "# Enregistrement des résultats sur un fichier json\n",
    "with open(\"output/results.json\", \"w\") as outfile:\n",
    "    json.dump(metric_over_time, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_iteration(dataset_path, target_labels,epochs):\n",
    "    results = []\n",
    "    for epoch in range(epochs):\n",
    "        model = NNWithMemory(learning_rate=0.001, feature_size=1006, hidden_sizes=300, label_size=20,sequence_length=1)\n",
    "        metric_over_time = []\n",
    "        metric = metrics.multioutput.SampleAverage(metrics.Jaccard())\n",
    "        #Début de l'évaluation sur le flux\n",
    "        for x, y in stream.iter_arff(dataset_path, target_labels, sparse=True): #Pr chaque data renvoi deux dico : x -> Ensemble de carac , y-> Emsemble de label\n",
    "            # Répartition des attributs sur un array numpy\n",
    "            new_x = np.zeros(1006)\n",
    "            for key, value in x.items():\n",
    "                x[key] = float(value) #Convertie la valeur de l'attribut en flottant\n",
    "                k = int(re.findall(r'\\d+',key)[0]) #\"X49\" -> \"49\".\n",
    "                new_x[k] = value\n",
    "            # Prédiction\n",
    "            y_pred = model.predict_one(new_x)\n",
    "            n = 0\n",
    "            # Convertion de y_pred en dictionnaire contenant des true et false pour la comparaison avec y\n",
    "            dict_y_pred = dict()\n",
    "            for j in y_pred :\n",
    "                if j == None : j = False\n",
    "                elif j >= 0.5 : j = True\n",
    "                else : j = False\n",
    "                dict_y_pred['y{}'.format(n)] = j\n",
    "                n+=1\n",
    "            # Assurance que le dictionnaire y ne contienne que des trues et false\n",
    "            for k, v in y.items():\n",
    "                y[k] = v == \"1\" #Convertit chaque label en booléen\n",
    "            # Répartition des labels de y sur un array numpy\n",
    "            new_y = np.zeros(20)\n",
    "            for key, value in y.items():\n",
    "                k = int(re.findall(r'\\d+',key)[0])\n",
    "                new_y[k] = value # Affecte la valeur booléenne (True ou False) convertie en 1.0 ou 0.0\n",
    "            # Mise à jour de la métrique avec les dictionnaires de y et de la prédiction\n",
    "            metric.update(y, dict_y_pred)\n",
    "            metric_over_time.append(metric.get())\n",
    "            # Apprentissage sur les array numpy de x et y\n",
    "            model.learn_one(new_x, new_y)\n",
    "\n",
    "        results.append(metric.get())\n",
    "        print(f\"Epoch : {epoch}, Jaccard Index: {metric.get()}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Appel de la fonction\n",
    "dataset_path = \"data/datasets/20NG.arff\"\n",
    "target_labels = [f'y{i}' for i in range(20)]\n",
    "results_ =LSTM_iteration(dataset_path, target_labels,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moy = np.mean(results_)\n",
    "print(moy)\n",
    "std = np.std(results_)\n",
    "print(std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp_river",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
